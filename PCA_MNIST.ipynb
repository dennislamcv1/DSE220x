{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal component analysis on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we get an understanding of **principal component analysis (PCA)** using the familiar MNIST data set of handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few routines check if the MNIST data is already in the current directory; if not, it is downloaded directly from Yann Le Cun's web site. It is then loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "import gzip, sys, os\n",
    "\n",
    "if sys.version_info[0] == 2:\n",
    "    from urllib import urlretrieve\n",
    "else:\n",
    "    from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "    print(\"Downloading %s\" % filename)\n",
    "    urlretrieve(source + filename, filename)\n",
    "\n",
    "def load_mnist_images(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        download(filename)\n",
    "    # Read the inputs in Yann LeCun's binary format.\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    data = data.reshape(-1,784)\n",
    "    return data / np.float32(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the training set\n",
    "train_data = load_mnist_images('train-images-idx3-ubyte.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Statistics of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis chooses projection directions based on the **covariance matrix** of the data. This matrix allows us to contrast the effect of picking coordinate directions (i.e. pixels) versus eigenvector directions. In particular:\n",
    "* *The ith **diagonal entry** of the covariance is the variance in the ith coordinate (the ith pixel).*\n",
    "* *The ith **eigenvalue** of the covariance matrix is the variance in the direction of the ith eigenvector.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute covariance matrix\n",
    "Sigma = np.cov(train_data, rowvar=0, bias=1)\n",
    "# Compute coordinate-wise variances, in increasing order\n",
    "coordinate_variances = np.sort(Sigma.diagonal())\n",
    "# Compute variances in eigenvector directions, in increasing order\n",
    "eigenvector_variances = np.sort(np.linalg.eigvalsh(Sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the (substantial) benefit of eigenvector projections over coordinate projections, we create a plot that shows the variance lost due to each of these.\n",
    "\n",
    "For each `k` (projection dimension), we compute:\n",
    "* How much of the overall variance is lost when we project to the best `k` coordinate directions?\n",
    "* How much of the overall variance is lost when we project to the top `k` eigenvectors (as in PCA)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute fraction of overall variance lost when projecting to k coordinate directions\n",
    "total_coordinate_variance = np.cumsum(coordinate_variances)\n",
    "total_coordinate_variance = total_coordinate_variance/total_coordinate_variance[783]\n",
    "# Compute fraction of overall variance lost when projecting to k eigenvector directions\n",
    "total_eigenvector_variance = np.cumsum(eigenvector_variances)\n",
    "total_eigenvector_variance = total_eigenvector_variance/total_eigenvector_variance[783]\n",
    "# Plot these results\n",
    "plt.plot(np.arange(1,784), total_coordinate_variance[784:0:-1], 'b-', lw=2)\n",
    "plt.plot(np.arange(1,784), total_eigenvector_variance[784:0:-1], 'r-', lw=2)\n",
    "plt.xlabel('projection dimension', fontsize=14)\n",
    "plt.ylabel('fraction of residual variance', fontsize=14)\n",
    "plt.xlim(0,784)\n",
    "plt.ylim(0.0,1.0)\n",
    "plt.legend(['coordinate directions', 'PCA directions'], fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Projection and reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get a more *visual* feel for what information is lost during dimensionality reduction.\n",
    "\n",
    "Suppose we find the PCA projection to `k` dimensions. What is the result of:\n",
    "* Starting with a handwritten digit in the original (784-dimensional) space\n",
    "* *Projecting* it down to `k` dimensions\n",
    "* *Reconstructing* an image in 784-dimensional space from this `k`-dimensional projection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by computing the eigenvalues and eigenvectors of the covariance matrix. The routine `numpy.linalg.eigh` returns these in order of increasing eigenvalue. The eigenvectors are normalized to unit length and returned as columns of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eigh(Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let `U` be the 784-by-`k` matrix whose columns are the top `k` eigenvectors. Then:\n",
    "* the matrix `transpose(U)` performs the PCA projection onto the top `k` directions\n",
    "* the matrix `U` reconstructs a point in the original space from its `k` dimensional projection\n",
    "\n",
    "Thus the product `U*transpose(U)` is a 784-by-784 matrix that does a *projection-followed-by-reconstruction*. The following function returns this matrix for any specified `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that returns the project-and-reconstruct operations as a single matrix\n",
    "def projection_and_reconstruction(k):\n",
    "    U = eigenvectors[:,(784-k):784]\n",
    "    P = np.dot(U,U.T)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next routine displays an handwritten digit image given as a 784-dimensional vector. It begins by clipping each entry to lie in the range [0,255]; the images returned after PCA reconstruction might not satisfy this property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_digit(x):\n",
    "    # Make sure all entries of x are in the range [0,255]\n",
    "    for i in range(784):\n",
    "        x[i] = max(0.0, x[i])\n",
    "        x[i] = min(255.0, x[i])\n",
    "    # Now display\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x.reshape((28,28)), cmap=plt.cm.gray)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally get to our main visualization function. It is invoked as follows:\n",
    "* **`show_effect_of_PCA(x, k_list)`**\n",
    "\n",
    "where:\n",
    "* `x` is the image of a handwritten digit (a 784-dimensional vector)\n",
    "* `k_list` is a list of projection dimensions (in the range 1 to 784)\n",
    "\n",
    "It displays the original image as well as the reconstructions after projecting to each of the specified dimensions using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_effect_of_PCA(x, k_list):\n",
    "    print \"Original:\"\n",
    "    show_digit(x)\n",
    "    for k in k_list:\n",
    "        if (k > 0) and (k < 784):\n",
    "            print \"Projection to dimension \", k\n",
    "            P = projection_and_reconstruction(k)\n",
    "            show_digit(P.dot(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out on the 1000th data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1000\n",
    "show_effect_of_PCA(train_data[index,], [100, 50, 25, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"magenta\">For you to do: Try plenty of other examples!</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "344px",
    "left": "1px",
    "right": "20px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
